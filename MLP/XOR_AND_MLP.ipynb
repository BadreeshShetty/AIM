{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"XOR_AND_MLP.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"TiEuYZ29LDDb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"outputId":"8ae69199-218a-47a1-b7be-ef94d925d8c3","executionInfo":{"status":"ok","timestamp":1546627553559,"user_tz":-330,"elapsed":1059,"user":{"displayName":"Badreesh Shetty","photoUrl":"","userId":"14102157331230489534"}}},"cell_type":"code","source":["#XOR Gate:\n","\n","import numpy as np \n","from sklearn.neural_network import MLPClassifier\n","\n","X=np.array([[0,0],[0,1],[1,0],[1,1]])\n","Y= np.array([0,1,1,0])\n","# print(X,Y)\n","\n","model = MLPClassifier(hidden_layer_sizes=(50,),activation = 'logistic',solver ='adam',max_iter=1000,verbose=1)\n","\n","model.fit(X,Y)\n","\n","predict= model.predict(X)\n","\n","print(predict)\n","\n","print(model.score(X,Y))\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Iteration 1, loss = 0.69410253\n","Iteration 2, loss = 0.69366215\n","Iteration 3, loss = 0.69343001\n","Iteration 4, loss = 0.69338139\n","Iteration 5, loss = 0.69344764\n","Iteration 6, loss = 0.69353131\n","Iteration 7, loss = 0.69356650\n","Iteration 8, loss = 0.69354373\n","Iteration 9, loss = 0.69348505\n","Iteration 10, loss = 0.69341882\n","Iteration 11, loss = 0.69336757\n","Iteration 12, loss = 0.69334311\n","Iteration 13, loss = 0.69334519\n","Iteration 14, loss = 0.69336350\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","[1 0 0 0]\n","0.25\n"],"name":"stdout"}]},{"metadata":{"id":"dYysErrTLG4z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":17105},"outputId":"7b20e72d-8f71-4a97-b2d4-ab32db7b59d6","executionInfo":{"status":"ok","timestamp":1546627554228,"user_tz":-330,"elapsed":1703,"user":{"displayName":"Badreesh Shetty","photoUrl":"","userId":"14102157331230489534"}}},"cell_type":"code","source":["#AND Gate:\n","\n","import numpy as np \n","from sklearn.neural_network import MLPClassifier\n","\n","X=np.array([[0,0],[0,1],[1,0],[1,1]])\n","Y= np.array([0,0,0,1])\n","# print(X,Y)\n","\n","model = MLPClassifier(hidden_layer_sizes=(50,),activation = 'logistic',solver ='adam',max_iter=1000,verbose=1)\n","\n","model.fit(X,Y)\n","\n","predict= model.predict(X)\n","\n","print(predict)\n","\n","print(model.score(X,Y))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Iteration 1, loss = 0.65969744\n","Iteration 2, loss = 0.65403338\n","Iteration 3, loss = 0.64854166\n","Iteration 4, loss = 0.64322470\n","Iteration 5, loss = 0.63808450\n","Iteration 6, loss = 0.63312262\n","Iteration 7, loss = 0.62834018\n","Iteration 8, loss = 0.62373780\n","Iteration 9, loss = 0.61931567\n","Iteration 10, loss = 0.61507349\n","Iteration 11, loss = 0.61101050\n","Iteration 12, loss = 0.60712549\n","Iteration 13, loss = 0.60341674\n","Iteration 14, loss = 0.59988211\n","Iteration 15, loss = 0.59651899\n","Iteration 16, loss = 0.59332433\n","Iteration 17, loss = 0.59029467\n","Iteration 18, loss = 0.58742615\n","Iteration 19, loss = 0.58471458\n","Iteration 20, loss = 0.58215537\n","Iteration 21, loss = 0.57974368\n","Iteration 22, loss = 0.57747436\n","Iteration 23, loss = 0.57534204\n","Iteration 24, loss = 0.57334114\n","Iteration 25, loss = 0.57146590\n","Iteration 26, loss = 0.56971044\n","Iteration 27, loss = 0.56806879\n","Iteration 28, loss = 0.56653490\n","Iteration 29, loss = 0.56510271\n","Iteration 30, loss = 0.56376617\n","Iteration 31, loss = 0.56251927\n","Iteration 32, loss = 0.56135610\n","Iteration 33, loss = 0.56027082\n","Iteration 34, loss = 0.55925777\n","Iteration 35, loss = 0.55831142\n","Iteration 36, loss = 0.55742642\n","Iteration 37, loss = 0.55659765\n","Iteration 38, loss = 0.55582019\n","Iteration 39, loss = 0.55508934\n","Iteration 40, loss = 0.55440068\n","Iteration 41, loss = 0.55375002\n","Iteration 42, loss = 0.55313342\n","Iteration 43, loss = 0.55254721\n","Iteration 44, loss = 0.55198797\n","Iteration 45, loss = 0.55145257\n","Iteration 46, loss = 0.55093809\n","Iteration 47, loss = 0.55044188\n","Iteration 48, loss = 0.54996153\n","Iteration 49, loss = 0.54949486\n","Iteration 50, loss = 0.54903991\n","Iteration 51, loss = 0.54859492\n","Iteration 52, loss = 0.54815834\n","Iteration 53, loss = 0.54772879\n","Iteration 54, loss = 0.54730509\n","Iteration 55, loss = 0.54688618\n","Iteration 56, loss = 0.54647116\n","Iteration 57, loss = 0.54605928\n","Iteration 58, loss = 0.54564988\n","Iteration 59, loss = 0.54524243\n","Iteration 60, loss = 0.54483648\n","Iteration 61, loss = 0.54443166\n","Iteration 62, loss = 0.54402769\n","Iteration 63, loss = 0.54362434\n","Iteration 64, loss = 0.54322143\n","Iteration 65, loss = 0.54281883\n","Iteration 66, loss = 0.54241645\n","Iteration 67, loss = 0.54201423\n","Iteration 68, loss = 0.54161212\n","Iteration 69, loss = 0.54121010\n","Iteration 70, loss = 0.54080816\n","Iteration 71, loss = 0.54040630\n","Iteration 72, loss = 0.54000452\n","Iteration 73, loss = 0.53960282\n","Iteration 74, loss = 0.53920123\n","Iteration 75, loss = 0.53879973\n","Iteration 76, loss = 0.53839833\n","Iteration 77, loss = 0.53799702\n","Iteration 78, loss = 0.53759580\n","Iteration 79, loss = 0.53719464\n","Iteration 80, loss = 0.53679353\n","Iteration 81, loss = 0.53639244\n","Iteration 82, loss = 0.53599134\n","Iteration 83, loss = 0.53559019\n","Iteration 84, loss = 0.53518893\n","Iteration 85, loss = 0.53478754\n","Iteration 86, loss = 0.53438595\n","Iteration 87, loss = 0.53398412\n","Iteration 88, loss = 0.53358198\n","Iteration 89, loss = 0.53317947\n","Iteration 90, loss = 0.53277654\n","Iteration 91, loss = 0.53237313\n","Iteration 92, loss = 0.53196917\n","Iteration 93, loss = 0.53156461\n","Iteration 94, loss = 0.53115939\n","Iteration 95, loss = 0.53075344\n","Iteration 96, loss = 0.53034672\n","Iteration 97, loss = 0.52993916\n","Iteration 98, loss = 0.52953071\n","Iteration 99, loss = 0.52912133\n","Iteration 100, loss = 0.52871095\n","Iteration 101, loss = 0.52829953\n","Iteration 102, loss = 0.52788704\n","Iteration 103, loss = 0.52747341\n","Iteration 104, loss = 0.52705862\n","Iteration 105, loss = 0.52664261\n","Iteration 106, loss = 0.52622536\n","Iteration 107, loss = 0.52580682\n","Iteration 108, loss = 0.52538697\n","Iteration 109, loss = 0.52496575\n","Iteration 110, loss = 0.52454315\n","Iteration 111, loss = 0.52411913\n","Iteration 112, loss = 0.52369366\n","Iteration 113, loss = 0.52326671\n","Iteration 114, loss = 0.52283825\n","Iteration 115, loss = 0.52240826\n","Iteration 116, loss = 0.52197669\n","Iteration 117, loss = 0.52154353\n","Iteration 118, loss = 0.52110875\n","Iteration 119, loss = 0.52067232\n","Iteration 120, loss = 0.52023422\n","Iteration 121, loss = 0.51979441\n","Iteration 122, loss = 0.51935288\n","Iteration 123, loss = 0.51890959\n","Iteration 124, loss = 0.51846452\n","Iteration 125, loss = 0.51801765\n","Iteration 126, loss = 0.51756894\n","Iteration 127, loss = 0.51711837\n","Iteration 128, loss = 0.51666593\n","Iteration 129, loss = 0.51621157\n","Iteration 130, loss = 0.51575528\n","Iteration 131, loss = 0.51529703\n","Iteration 132, loss = 0.51483680\n","Iteration 133, loss = 0.51437456\n","Iteration 134, loss = 0.51391029\n","Iteration 135, loss = 0.51344396\n","Iteration 136, loss = 0.51297555\n","Iteration 137, loss = 0.51250503\n","Iteration 138, loss = 0.51203238\n","Iteration 139, loss = 0.51155758\n","Iteration 140, loss = 0.51108060\n","Iteration 141, loss = 0.51060142\n","Iteration 142, loss = 0.51012002\n","Iteration 143, loss = 0.50963638\n","Iteration 144, loss = 0.50915046\n","Iteration 145, loss = 0.50866226\n","Iteration 146, loss = 0.50817174\n","Iteration 147, loss = 0.50767888\n","Iteration 148, loss = 0.50718368\n","Iteration 149, loss = 0.50668609\n","Iteration 150, loss = 0.50618610\n","Iteration 151, loss = 0.50568370\n","Iteration 152, loss = 0.50517885\n","Iteration 153, loss = 0.50467155\n","Iteration 154, loss = 0.50416176\n","Iteration 155, loss = 0.50364947\n","Iteration 156, loss = 0.50313467\n","Iteration 157, loss = 0.50261732\n","Iteration 158, loss = 0.50209741\n","Iteration 159, loss = 0.50157492\n","Iteration 160, loss = 0.50104984\n","Iteration 161, loss = 0.50052214\n","Iteration 162, loss = 0.49999181\n","Iteration 163, loss = 0.49945883\n","Iteration 164, loss = 0.49892318\n","Iteration 165, loss = 0.49838484\n","Iteration 166, loss = 0.49784379\n","Iteration 167, loss = 0.49730003\n","Iteration 168, loss = 0.49675353\n","Iteration 169, loss = 0.49620427\n","Iteration 170, loss = 0.49565224\n","Iteration 171, loss = 0.49509743\n","Iteration 172, loss = 0.49453982\n","Iteration 173, loss = 0.49397939\n","Iteration 174, loss = 0.49341612\n","Iteration 175, loss = 0.49285002\n","Iteration 176, loss = 0.49228105\n","Iteration 177, loss = 0.49170920\n","Iteration 178, loss = 0.49113447\n","Iteration 179, loss = 0.49055683\n","Iteration 180, loss = 0.48997628\n","Iteration 181, loss = 0.48939280\n","Iteration 182, loss = 0.48880638\n","Iteration 183, loss = 0.48821700\n","Iteration 184, loss = 0.48762466\n","Iteration 185, loss = 0.48702935\n","Iteration 186, loss = 0.48643104\n","Iteration 187, loss = 0.48582974\n","Iteration 188, loss = 0.48522542\n","Iteration 189, loss = 0.48461809\n","Iteration 190, loss = 0.48400772\n","Iteration 191, loss = 0.48339432\n","Iteration 192, loss = 0.48277786\n","Iteration 193, loss = 0.48215835\n","Iteration 194, loss = 0.48153577\n","Iteration 195, loss = 0.48091012\n","Iteration 196, loss = 0.48028138\n","Iteration 197, loss = 0.47964955\n","Iteration 198, loss = 0.47901462\n","Iteration 199, loss = 0.47837659\n","Iteration 200, loss = 0.47773544\n","Iteration 201, loss = 0.47709118\n","Iteration 202, loss = 0.47644379\n","Iteration 203, loss = 0.47579328\n","Iteration 204, loss = 0.47513963\n","Iteration 205, loss = 0.47448284\n","Iteration 206, loss = 0.47382290\n","Iteration 207, loss = 0.47315982\n","Iteration 208, loss = 0.47249359\n","Iteration 209, loss = 0.47182420\n","Iteration 210, loss = 0.47115166\n","Iteration 211, loss = 0.47047595\n","Iteration 212, loss = 0.46979708\n","Iteration 213, loss = 0.46911505\n","Iteration 214, loss = 0.46842985\n","Iteration 215, loss = 0.46774148\n","Iteration 216, loss = 0.46704995\n","Iteration 217, loss = 0.46635525\n","Iteration 218, loss = 0.46565738\n","Iteration 219, loss = 0.46495634\n","Iteration 220, loss = 0.46425214\n","Iteration 221, loss = 0.46354477\n","Iteration 222, loss = 0.46283424\n","Iteration 223, loss = 0.46212054\n","Iteration 224, loss = 0.46140369\n","Iteration 225, loss = 0.46068368\n","Iteration 226, loss = 0.45996052\n","Iteration 227, loss = 0.45923421\n","Iteration 228, loss = 0.45850476\n","Iteration 229, loss = 0.45777216\n","Iteration 230, loss = 0.45703643\n","Iteration 231, loss = 0.45629757\n","Iteration 232, loss = 0.45555558\n","Iteration 233, loss = 0.45481047\n","Iteration 234, loss = 0.45406226\n","Iteration 235, loss = 0.45331093\n","Iteration 236, loss = 0.45255651\n","Iteration 237, loss = 0.45179900\n","Iteration 238, loss = 0.45103841\n","Iteration 239, loss = 0.45027475\n","Iteration 240, loss = 0.44950802\n","Iteration 241, loss = 0.44873823\n","Iteration 242, loss = 0.44796540\n","Iteration 243, loss = 0.44718954\n","Iteration 244, loss = 0.44641065\n","Iteration 245, loss = 0.44562875\n","Iteration 246, loss = 0.44484384\n","Iteration 247, loss = 0.44405595\n","Iteration 248, loss = 0.44326508\n","Iteration 249, loss = 0.44247124\n","Iteration 250, loss = 0.44167446\n","Iteration 251, loss = 0.44087473\n","Iteration 252, loss = 0.44007208\n","Iteration 253, loss = 0.43926652\n","Iteration 254, loss = 0.43845807\n","Iteration 255, loss = 0.43764673\n","Iteration 256, loss = 0.43683254\n","Iteration 257, loss = 0.43601549\n","Iteration 258, loss = 0.43519561\n","Iteration 259, loss = 0.43437292\n","Iteration 260, loss = 0.43354743\n","Iteration 261, loss = 0.43271916\n","Iteration 262, loss = 0.43188813\n","Iteration 263, loss = 0.43105435\n","Iteration 264, loss = 0.43021785\n","Iteration 265, loss = 0.42937864\n","Iteration 266, loss = 0.42853675\n","Iteration 267, loss = 0.42769219\n","Iteration 268, loss = 0.42684498\n","Iteration 269, loss = 0.42599515\n","Iteration 270, loss = 0.42514271\n","Iteration 271, loss = 0.42428769\n","Iteration 272, loss = 0.42343011\n","Iteration 273, loss = 0.42256999\n","Iteration 274, loss = 0.42170736\n","Iteration 275, loss = 0.42084223\n","Iteration 276, loss = 0.41997463\n","Iteration 277, loss = 0.41910458\n","Iteration 278, loss = 0.41823211\n","Iteration 279, loss = 0.41735724\n","Iteration 280, loss = 0.41648000\n","Iteration 281, loss = 0.41560041\n","Iteration 282, loss = 0.41471849\n","Iteration 283, loss = 0.41383428\n","Iteration 284, loss = 0.41294779\n","Iteration 285, loss = 0.41205905\n","Iteration 286, loss = 0.41116810\n","Iteration 287, loss = 0.41027495\n","Iteration 288, loss = 0.40937964\n","Iteration 289, loss = 0.40848219\n","Iteration 290, loss = 0.40758263\n","Iteration 291, loss = 0.40668098\n","Iteration 292, loss = 0.40577728\n","Iteration 293, loss = 0.40487155\n","Iteration 294, loss = 0.40396382\n","Iteration 295, loss = 0.40305413\n","Iteration 296, loss = 0.40214249\n","Iteration 297, loss = 0.40122895\n","Iteration 298, loss = 0.40031352\n","Iteration 299, loss = 0.39939625\n","Iteration 300, loss = 0.39847715\n","Iteration 301, loss = 0.39755626\n","Iteration 302, loss = 0.39663361\n","Iteration 303, loss = 0.39570924\n","Iteration 304, loss = 0.39478316\n","Iteration 305, loss = 0.39385542\n","Iteration 306, loss = 0.39292604\n","Iteration 307, loss = 0.39199506\n","Iteration 308, loss = 0.39106250\n","Iteration 309, loss = 0.39012841\n","Iteration 310, loss = 0.38919281\n","Iteration 311, loss = 0.38825573\n","Iteration 312, loss = 0.38731720\n","Iteration 313, loss = 0.38637726\n","Iteration 314, loss = 0.38543595\n","Iteration 315, loss = 0.38449328\n","Iteration 316, loss = 0.38354931\n","Iteration 317, loss = 0.38260405\n","Iteration 318, loss = 0.38165754\n","Iteration 319, loss = 0.38070982\n","Iteration 320, loss = 0.37976091\n","Iteration 321, loss = 0.37881086\n","Iteration 322, loss = 0.37785969\n","Iteration 323, loss = 0.37690743\n","Iteration 324, loss = 0.37595413\n","Iteration 325, loss = 0.37499982\n","Iteration 326, loss = 0.37404452\n","Iteration 327, loss = 0.37308827\n","Iteration 328, loss = 0.37213110\n","Iteration 329, loss = 0.37117306\n","Iteration 330, loss = 0.37021417\n","Iteration 331, loss = 0.36925446\n","Iteration 332, loss = 0.36829397\n","Iteration 333, loss = 0.36733273\n","Iteration 334, loss = 0.36637078\n","Iteration 335, loss = 0.36540815\n","Iteration 336, loss = 0.36444487\n","Iteration 337, loss = 0.36348098\n","Iteration 338, loss = 0.36251651\n","Iteration 339, loss = 0.36155149\n","Iteration 340, loss = 0.36058596\n","Iteration 341, loss = 0.35961995\n","Iteration 342, loss = 0.35865349\n","Iteration 343, loss = 0.35768661\n","Iteration 344, loss = 0.35671936\n","Iteration 345, loss = 0.35575176\n","Iteration 346, loss = 0.35478384\n","Iteration 347, loss = 0.35381564\n","Iteration 348, loss = 0.35284719\n","Iteration 349, loss = 0.35187852\n","Iteration 350, loss = 0.35090967\n","Iteration 351, loss = 0.34994066\n","Iteration 352, loss = 0.34897154\n","Iteration 353, loss = 0.34800232\n","Iteration 354, loss = 0.34703305\n","Iteration 355, loss = 0.34606375\n","Iteration 356, loss = 0.34509446\n","Iteration 357, loss = 0.34412521\n","Iteration 358, loss = 0.34315603\n","Iteration 359, loss = 0.34218695\n","Iteration 360, loss = 0.34121800\n","Iteration 361, loss = 0.34024921\n","Iteration 362, loss = 0.33928061\n","Iteration 363, loss = 0.33831224\n","Iteration 364, loss = 0.33734412\n","Iteration 365, loss = 0.33637629\n","Iteration 366, loss = 0.33540877\n","Iteration 367, loss = 0.33444159\n","Iteration 368, loss = 0.33347478\n","Iteration 369, loss = 0.33250838\n","Iteration 370, loss = 0.33154241\n","Iteration 371, loss = 0.33057689\n","Iteration 372, loss = 0.32961187\n","Iteration 373, loss = 0.32864736\n","Iteration 374, loss = 0.32768340\n","Iteration 375, loss = 0.32672001\n","Iteration 376, loss = 0.32575722\n","Iteration 377, loss = 0.32479506\n","Iteration 378, loss = 0.32383355\n","Iteration 379, loss = 0.32287273\n","Iteration 380, loss = 0.32191261\n","Iteration 381, loss = 0.32095323\n","Iteration 382, loss = 0.31999462\n","Iteration 383, loss = 0.31903679\n","Iteration 384, loss = 0.31807977\n","Iteration 385, loss = 0.31712360\n","Iteration 386, loss = 0.31616829\n","Iteration 387, loss = 0.31521387\n","Iteration 388, loss = 0.31426036\n","Iteration 389, loss = 0.31330780\n","Iteration 390, loss = 0.31235619\n","Iteration 391, loss = 0.31140558\n","Iteration 392, loss = 0.31045598\n","Iteration 393, loss = 0.30950741\n","Iteration 394, loss = 0.30855990\n","Iteration 395, loss = 0.30761347\n","Iteration 396, loss = 0.30666815\n","Iteration 397, loss = 0.30572395\n","Iteration 398, loss = 0.30478090\n","Iteration 399, loss = 0.30383903\n","Iteration 400, loss = 0.30289834\n","Iteration 401, loss = 0.30195887\n","Iteration 402, loss = 0.30102063\n","Iteration 403, loss = 0.30008365\n","Iteration 404, loss = 0.29914795\n","Iteration 405, loss = 0.29821355\n","Iteration 406, loss = 0.29728046\n","Iteration 407, loss = 0.29634870\n","Iteration 408, loss = 0.29541831\n","Iteration 409, loss = 0.29448929\n","Iteration 410, loss = 0.29356167\n","Iteration 411, loss = 0.29263545\n","Iteration 412, loss = 0.29171068\n","Iteration 413, loss = 0.29078735\n","Iteration 414, loss = 0.28986549\n","Iteration 415, loss = 0.28894511\n","Iteration 416, loss = 0.28802624\n","Iteration 417, loss = 0.28710890\n","Iteration 418, loss = 0.28619308\n","Iteration 419, loss = 0.28527883\n","Iteration 420, loss = 0.28436614\n","Iteration 421, loss = 0.28345504\n","Iteration 422, loss = 0.28254555\n","Iteration 423, loss = 0.28163767\n","Iteration 424, loss = 0.28073142\n","Iteration 425, loss = 0.27982683\n","Iteration 426, loss = 0.27892390\n","Iteration 427, loss = 0.27802265\n","Iteration 428, loss = 0.27712309\n","Iteration 429, loss = 0.27622524\n","Iteration 430, loss = 0.27532911\n","Iteration 431, loss = 0.27443471\n","Iteration 432, loss = 0.27354206\n","Iteration 433, loss = 0.27265118\n","Iteration 434, loss = 0.27176207\n","Iteration 435, loss = 0.27087474\n","Iteration 436, loss = 0.26998922\n","Iteration 437, loss = 0.26910550\n","Iteration 438, loss = 0.26822361\n","Iteration 439, loss = 0.26734356\n","Iteration 440, loss = 0.26646535\n","Iteration 441, loss = 0.26558900\n","Iteration 442, loss = 0.26471452\n","Iteration 443, loss = 0.26384192\n","Iteration 444, loss = 0.26297121\n","Iteration 445, loss = 0.26210240\n","Iteration 446, loss = 0.26123550\n","Iteration 447, loss = 0.26037052\n","Iteration 448, loss = 0.25950748\n","Iteration 449, loss = 0.25864637\n","Iteration 450, loss = 0.25778721\n","Iteration 451, loss = 0.25693002\n","Iteration 452, loss = 0.25607479\n","Iteration 453, loss = 0.25522154\n","Iteration 454, loss = 0.25437027\n","Iteration 455, loss = 0.25352099\n","Iteration 456, loss = 0.25267372\n","Iteration 457, loss = 0.25182846\n","Iteration 458, loss = 0.25098521\n","Iteration 459, loss = 0.25014399\n","Iteration 460, loss = 0.24930479\n","Iteration 461, loss = 0.24846764\n","Iteration 462, loss = 0.24763253\n","Iteration 463, loss = 0.24679948\n","Iteration 464, loss = 0.24596848\n","Iteration 465, loss = 0.24513955\n","Iteration 466, loss = 0.24431269\n","Iteration 467, loss = 0.24348791\n","Iteration 468, loss = 0.24266522\n","Iteration 469, loss = 0.24184461\n","Iteration 470, loss = 0.24102609\n","Iteration 471, loss = 0.24020968\n","Iteration 472, loss = 0.23939537\n","Iteration 473, loss = 0.23858317\n","Iteration 474, loss = 0.23777309\n","Iteration 475, loss = 0.23696512\n","Iteration 476, loss = 0.23615928\n","Iteration 477, loss = 0.23535557\n","Iteration 478, loss = 0.23455399\n","Iteration 479, loss = 0.23375455\n","Iteration 480, loss = 0.23295725\n","Iteration 481, loss = 0.23216209\n","Iteration 482, loss = 0.23136909\n","Iteration 483, loss = 0.23057823\n","Iteration 484, loss = 0.22978953\n","Iteration 485, loss = 0.22900298\n","Iteration 486, loss = 0.22821860\n","Iteration 487, loss = 0.22743638\n","Iteration 488, loss = 0.22665633\n","Iteration 489, loss = 0.22587845\n","Iteration 490, loss = 0.22510273\n","Iteration 491, loss = 0.22432920\n","Iteration 492, loss = 0.22355784\n","Iteration 493, loss = 0.22278866\n","Iteration 494, loss = 0.22202166\n","Iteration 495, loss = 0.22125684\n","Iteration 496, loss = 0.22049421\n","Iteration 497, loss = 0.21973376\n","Iteration 498, loss = 0.21897550\n","Iteration 499, loss = 0.21821943\n","Iteration 500, loss = 0.21746555\n","Iteration 501, loss = 0.21671386\n","Iteration 502, loss = 0.21596437\n","Iteration 503, loss = 0.21521706\n","Iteration 504, loss = 0.21447196\n","Iteration 505, loss = 0.21372904\n","Iteration 506, loss = 0.21298833\n","Iteration 507, loss = 0.21224981\n","Iteration 508, loss = 0.21151348\n","Iteration 509, loss = 0.21077936\n","Iteration 510, loss = 0.21004743\n","Iteration 511, loss = 0.20931770\n","Iteration 512, loss = 0.20859017\n","Iteration 513, loss = 0.20786483\n","Iteration 514, loss = 0.20714169\n","Iteration 515, loss = 0.20642075\n","Iteration 516, loss = 0.20570201\n","Iteration 517, loss = 0.20498546\n","Iteration 518, loss = 0.20427111\n","Iteration 519, loss = 0.20355896\n","Iteration 520, loss = 0.20284900\n","Iteration 521, loss = 0.20214124\n","Iteration 522, loss = 0.20143567\n","Iteration 523, loss = 0.20073229\n","Iteration 524, loss = 0.20003110\n","Iteration 525, loss = 0.19933211\n","Iteration 526, loss = 0.19863530\n","Iteration 527, loss = 0.19794069\n","Iteration 528, loss = 0.19724826\n","Iteration 529, loss = 0.19655801\n","Iteration 530, loss = 0.19586995\n","Iteration 531, loss = 0.19518408\n","Iteration 532, loss = 0.19450039\n","Iteration 533, loss = 0.19381887\n","Iteration 534, loss = 0.19313954\n","Iteration 535, loss = 0.19246238\n","Iteration 536, loss = 0.19178739\n","Iteration 537, loss = 0.19111458\n","Iteration 538, loss = 0.19044394\n","Iteration 539, loss = 0.18977547\n","Iteration 540, loss = 0.18910917\n","Iteration 541, loss = 0.18844503\n","Iteration 542, loss = 0.18778305\n","Iteration 543, loss = 0.18712324\n","Iteration 544, loss = 0.18646558\n","Iteration 545, loss = 0.18581008\n","Iteration 546, loss = 0.18515673\n","Iteration 547, loss = 0.18450554\n","Iteration 548, loss = 0.18385649\n","Iteration 549, loss = 0.18320959\n","Iteration 550, loss = 0.18256483\n","Iteration 551, loss = 0.18192222\n","Iteration 552, loss = 0.18128174\n","Iteration 553, loss = 0.18064340\n","Iteration 554, loss = 0.18000719\n","Iteration 555, loss = 0.17937311\n","Iteration 556, loss = 0.17874115\n","Iteration 557, loss = 0.17811133\n","Iteration 558, loss = 0.17748362\n","Iteration 559, loss = 0.17685803\n","Iteration 560, loss = 0.17623455\n","Iteration 561, loss = 0.17561319\n","Iteration 562, loss = 0.17499394\n","Iteration 563, loss = 0.17437679\n","Iteration 564, loss = 0.17376175\n","Iteration 565, loss = 0.17314880\n","Iteration 566, loss = 0.17253795\n","Iteration 567, loss = 0.17192919\n","Iteration 568, loss = 0.17132252\n","Iteration 569, loss = 0.17071794\n","Iteration 570, loss = 0.17011544\n","Iteration 571, loss = 0.16951502\n","Iteration 572, loss = 0.16891667\n","Iteration 573, loss = 0.16832039\n","Iteration 574, loss = 0.16772619\n","Iteration 575, loss = 0.16713404\n","Iteration 576, loss = 0.16654396\n","Iteration 577, loss = 0.16595594\n","Iteration 578, loss = 0.16536996\n","Iteration 579, loss = 0.16478604\n","Iteration 580, loss = 0.16420416\n","Iteration 581, loss = 0.16362433\n","Iteration 582, loss = 0.16304653\n","Iteration 583, loss = 0.16247077\n","Iteration 584, loss = 0.16189704\n","Iteration 585, loss = 0.16132533\n","Iteration 586, loss = 0.16075565\n","Iteration 587, loss = 0.16018798\n","Iteration 588, loss = 0.15962233\n","Iteration 589, loss = 0.15905868\n","Iteration 590, loss = 0.15849705\n","Iteration 591, loss = 0.15793741\n","Iteration 592, loss = 0.15737978\n","Iteration 593, loss = 0.15682414\n","Iteration 594, loss = 0.15627048\n","Iteration 595, loss = 0.15571881\n","Iteration 596, loss = 0.15516913\n","Iteration 597, loss = 0.15462142\n","Iteration 598, loss = 0.15407568\n","Iteration 599, loss = 0.15353191\n","Iteration 600, loss = 0.15299010\n","Iteration 601, loss = 0.15245025\n","Iteration 602, loss = 0.15191236\n","Iteration 603, loss = 0.15137642\n","Iteration 604, loss = 0.15084242\n","Iteration 605, loss = 0.15031037\n","Iteration 606, loss = 0.14978025\n","Iteration 607, loss = 0.14925206\n","Iteration 608, loss = 0.14872581\n","Iteration 609, loss = 0.14820148\n","Iteration 610, loss = 0.14767906\n","Iteration 611, loss = 0.14715856\n","Iteration 612, loss = 0.14663997\n","Iteration 613, loss = 0.14612329\n","Iteration 614, loss = 0.14560851\n","Iteration 615, loss = 0.14509562\n","Iteration 616, loss = 0.14458462\n","Iteration 617, loss = 0.14407552\n","Iteration 618, loss = 0.14356829\n","Iteration 619, loss = 0.14306294\n","Iteration 620, loss = 0.14255946\n","Iteration 621, loss = 0.14205785\n","Iteration 622, loss = 0.14155811\n","Iteration 623, loss = 0.14106022\n","Iteration 624, loss = 0.14056419\n","Iteration 625, loss = 0.14007001\n","Iteration 626, loss = 0.13957767\n","Iteration 627, loss = 0.13908716\n","Iteration 628, loss = 0.13859850\n","Iteration 629, loss = 0.13811166\n","Iteration 630, loss = 0.13762665\n","Iteration 631, loss = 0.13714346\n","Iteration 632, loss = 0.13666208\n","Iteration 633, loss = 0.13618251\n","Iteration 634, loss = 0.13570475\n","Iteration 635, loss = 0.13522879\n","Iteration 636, loss = 0.13475462\n","Iteration 637, loss = 0.13428225\n","Iteration 638, loss = 0.13381166\n","Iteration 639, loss = 0.13334285\n","Iteration 640, loss = 0.13287581\n","Iteration 641, loss = 0.13241055\n","Iteration 642, loss = 0.13194705\n","Iteration 643, loss = 0.13148531\n","Iteration 644, loss = 0.13102533\n","Iteration 645, loss = 0.13056710\n","Iteration 646, loss = 0.13011062\n","Iteration 647, loss = 0.12965587\n","Iteration 648, loss = 0.12920286\n","Iteration 649, loss = 0.12875158\n","Iteration 650, loss = 0.12830203\n","Iteration 651, loss = 0.12785420\n","Iteration 652, loss = 0.12740808\n","Iteration 653, loss = 0.12696367\n","Iteration 654, loss = 0.12652097\n","Iteration 655, loss = 0.12607997\n","Iteration 656, loss = 0.12564066\n","Iteration 657, loss = 0.12520305\n","Iteration 658, loss = 0.12476712\n","Iteration 659, loss = 0.12433287\n","Iteration 660, loss = 0.12390029\n","Iteration 661, loss = 0.12346938\n","Iteration 662, loss = 0.12304014\n","Iteration 663, loss = 0.12261256\n","Iteration 664, loss = 0.12218663\n","Iteration 665, loss = 0.12176235\n","Iteration 666, loss = 0.12133972\n","Iteration 667, loss = 0.12091872\n","Iteration 668, loss = 0.12049936\n","Iteration 669, loss = 0.12008162\n","Iteration 670, loss = 0.11966551\n","Iteration 671, loss = 0.11925102\n","Iteration 672, loss = 0.11883814\n","Iteration 673, loss = 0.11842687\n","Iteration 674, loss = 0.11801721\n","Iteration 675, loss = 0.11760914\n","Iteration 676, loss = 0.11720266\n","Iteration 677, loss = 0.11679777\n","Iteration 678, loss = 0.11639447\n","Iteration 679, loss = 0.11599274\n","Iteration 680, loss = 0.11559258\n","Iteration 681, loss = 0.11519399\n","Iteration 682, loss = 0.11479697\n","Iteration 683, loss = 0.11440150\n","Iteration 684, loss = 0.11400758\n","Iteration 685, loss = 0.11361520\n","Iteration 686, loss = 0.11322437\n","Iteration 687, loss = 0.11283508\n","Iteration 688, loss = 0.11244732\n","Iteration 689, loss = 0.11206108\n","Iteration 690, loss = 0.11167636\n","Iteration 691, loss = 0.11129316\n","Iteration 692, loss = 0.11091147\n","Iteration 693, loss = 0.11053129\n","Iteration 694, loss = 0.11015260\n","Iteration 695, loss = 0.10977541\n","Iteration 696, loss = 0.10939971\n","Iteration 697, loss = 0.10902550\n","Iteration 698, loss = 0.10865277\n","Iteration 699, loss = 0.10828151\n","Iteration 700, loss = 0.10791172\n","Iteration 701, loss = 0.10754339\n","Iteration 702, loss = 0.10717653\n","Iteration 703, loss = 0.10681112\n","Iteration 704, loss = 0.10644716\n","Iteration 705, loss = 0.10608464\n","Iteration 706, loss = 0.10572356\n","Iteration 707, loss = 0.10536392\n","Iteration 708, loss = 0.10500570\n","Iteration 709, loss = 0.10464891\n","Iteration 710, loss = 0.10429354\n","Iteration 711, loss = 0.10393958\n","Iteration 712, loss = 0.10358703\n","Iteration 713, loss = 0.10323589\n","Iteration 714, loss = 0.10288614\n","Iteration 715, loss = 0.10253779\n","Iteration 716, loss = 0.10219082\n","Iteration 717, loss = 0.10184524\n","Iteration 718, loss = 0.10150104\n","Iteration 719, loss = 0.10115821\n","Iteration 720, loss = 0.10081675\n","Iteration 721, loss = 0.10047666\n","Iteration 722, loss = 0.10013792\n","Iteration 723, loss = 0.09980053\n","Iteration 724, loss = 0.09946450\n","Iteration 725, loss = 0.09912981\n","Iteration 726, loss = 0.09879646\n","Iteration 727, loss = 0.09846444\n","Iteration 728, loss = 0.09813375\n","Iteration 729, loss = 0.09780438\n","Iteration 730, loss = 0.09747634\n","Iteration 731, loss = 0.09714961\n","Iteration 732, loss = 0.09682418\n","Iteration 733, loss = 0.09650007\n","Iteration 734, loss = 0.09617725\n","Iteration 735, loss = 0.09585572\n","Iteration 736, loss = 0.09553549\n","Iteration 737, loss = 0.09521654\n","Iteration 738, loss = 0.09489887\n","Iteration 739, loss = 0.09458247\n","Iteration 740, loss = 0.09426735\n","Iteration 741, loss = 0.09395349\n","Iteration 742, loss = 0.09364089\n","Iteration 743, loss = 0.09332955\n","Iteration 744, loss = 0.09301946\n","Iteration 745, loss = 0.09271062\n","Iteration 746, loss = 0.09240301\n","Iteration 747, loss = 0.09209665\n","Iteration 748, loss = 0.09179151\n","Iteration 749, loss = 0.09148760\n","Iteration 750, loss = 0.09118492\n","Iteration 751, loss = 0.09088345\n","Iteration 752, loss = 0.09058320\n","Iteration 753, loss = 0.09028415\n","Iteration 754, loss = 0.08998631\n","Iteration 755, loss = 0.08968966\n","Iteration 756, loss = 0.08939421\n","Iteration 757, loss = 0.08909995\n","Iteration 758, loss = 0.08880688\n","Iteration 759, loss = 0.08851498\n","Iteration 760, loss = 0.08822426\n","Iteration 761, loss = 0.08793472\n","Iteration 762, loss = 0.08764633\n","Iteration 763, loss = 0.08735911\n","Iteration 764, loss = 0.08707305\n","Iteration 765, loss = 0.08678814\n","Iteration 766, loss = 0.08650438\n","Iteration 767, loss = 0.08622176\n","Iteration 768, loss = 0.08594028\n","Iteration 769, loss = 0.08565993\n","Iteration 770, loss = 0.08538071\n","Iteration 771, loss = 0.08510262\n","Iteration 772, loss = 0.08482565\n","Iteration 773, loss = 0.08454980\n","Iteration 774, loss = 0.08427506\n","Iteration 775, loss = 0.08400142\n","Iteration 776, loss = 0.08372889\n","Iteration 777, loss = 0.08345746\n","Iteration 778, loss = 0.08318712\n","Iteration 779, loss = 0.08291787\n","Iteration 780, loss = 0.08264970\n","Iteration 781, loss = 0.08238262\n","Iteration 782, loss = 0.08211661\n","Iteration 783, loss = 0.08185168\n","Iteration 784, loss = 0.08158781\n","Iteration 785, loss = 0.08132501\n","Iteration 786, loss = 0.08106326\n","Iteration 787, loss = 0.08080257\n","Iteration 788, loss = 0.08054293\n","Iteration 789, loss = 0.08028434\n","Iteration 790, loss = 0.08002678\n","Iteration 791, loss = 0.07977027\n","Iteration 792, loss = 0.07951479\n","Iteration 793, loss = 0.07926033\n","Iteration 794, loss = 0.07900691\n","Iteration 795, loss = 0.07875450\n","Iteration 796, loss = 0.07850311\n","Iteration 797, loss = 0.07825273\n","Iteration 798, loss = 0.07800336\n","Iteration 799, loss = 0.07775499\n","Iteration 800, loss = 0.07750762\n","Iteration 801, loss = 0.07726125\n","Iteration 802, loss = 0.07701587\n","Iteration 803, loss = 0.07677148\n","Iteration 804, loss = 0.07652807\n","Iteration 805, loss = 0.07628564\n","Iteration 806, loss = 0.07604418\n","Iteration 807, loss = 0.07580370\n","Iteration 808, loss = 0.07556418\n","Iteration 809, loss = 0.07532562\n","Iteration 810, loss = 0.07508803\n","Iteration 811, loss = 0.07485138\n","Iteration 812, loss = 0.07461569\n","Iteration 813, loss = 0.07438095\n","Iteration 814, loss = 0.07414715\n","Iteration 815, loss = 0.07391428\n","Iteration 816, loss = 0.07368236\n","Iteration 817, loss = 0.07345136\n","Iteration 818, loss = 0.07322129\n","Iteration 819, loss = 0.07299214\n","Iteration 820, loss = 0.07276392\n","Iteration 821, loss = 0.07253660\n","Iteration 822, loss = 0.07231020\n","Iteration 823, loss = 0.07208471\n","Iteration 824, loss = 0.07186012\n","Iteration 825, loss = 0.07163643\n","Iteration 826, loss = 0.07141364\n","Iteration 827, loss = 0.07119174\n","Iteration 828, loss = 0.07097073\n","Iteration 829, loss = 0.07075060\n","Iteration 830, loss = 0.07053135\n","Iteration 831, loss = 0.07031298\n","Iteration 832, loss = 0.07009549\n","Iteration 833, loss = 0.06987886\n","Iteration 834, loss = 0.06966310\n","Iteration 835, loss = 0.06944821\n","Iteration 836, loss = 0.06923417\n","Iteration 837, loss = 0.06902099\n","Iteration 838, loss = 0.06880865\n","Iteration 839, loss = 0.06859717\n","Iteration 840, loss = 0.06838653\n","Iteration 841, loss = 0.06817673\n","Iteration 842, loss = 0.06796777\n","Iteration 843, loss = 0.06775964\n","Iteration 844, loss = 0.06755234\n","Iteration 845, loss = 0.06734587\n","Iteration 846, loss = 0.06714022\n","Iteration 847, loss = 0.06693538\n","Iteration 848, loss = 0.06673137\n","Iteration 849, loss = 0.06652817\n","Iteration 850, loss = 0.06632577\n","Iteration 851, loss = 0.06612418\n","Iteration 852, loss = 0.06592339\n","Iteration 853, loss = 0.06572340\n","Iteration 854, loss = 0.06552421\n","Iteration 855, loss = 0.06532580\n","Iteration 856, loss = 0.06512819\n","Iteration 857, loss = 0.06493135\n","Iteration 858, loss = 0.06473530\n","Iteration 859, loss = 0.06454003\n","Iteration 860, loss = 0.06434553\n","Iteration 861, loss = 0.06415180\n","Iteration 862, loss = 0.06395884\n","Iteration 863, loss = 0.06376665\n","Iteration 864, loss = 0.06357521\n","Iteration 865, loss = 0.06338454\n","Iteration 866, loss = 0.06319461\n","Iteration 867, loss = 0.06300544\n","Iteration 868, loss = 0.06281702\n","Iteration 869, loss = 0.06262934\n","Iteration 870, loss = 0.06244240\n","Iteration 871, loss = 0.06225621\n","Iteration 872, loss = 0.06207074\n","Iteration 873, loss = 0.06188601\n","Iteration 874, loss = 0.06170200\n","Iteration 875, loss = 0.06151873\n","Iteration 876, loss = 0.06133617\n","Iteration 877, loss = 0.06115433\n","Iteration 878, loss = 0.06097321\n","Iteration 879, loss = 0.06079280\n","Iteration 880, loss = 0.06061311\n","Iteration 881, loss = 0.06043412\n","Iteration 882, loss = 0.06025583\n","Iteration 883, loss = 0.06007824\n","Iteration 884, loss = 0.05990135\n","Iteration 885, loss = 0.05972515\n","Iteration 886, loss = 0.05954965\n","Iteration 887, loss = 0.05937483\n","Iteration 888, loss = 0.05920070\n","Iteration 889, loss = 0.05902726\n","Iteration 890, loss = 0.05885449\n","Iteration 891, loss = 0.05868239\n","Iteration 892, loss = 0.05851098\n","Iteration 893, loss = 0.05834023\n","Iteration 894, loss = 0.05817015\n","Iteration 895, loss = 0.05800073\n","Iteration 896, loss = 0.05783198\n","Iteration 897, loss = 0.05766388\n","Iteration 898, loss = 0.05749644\n","Iteration 899, loss = 0.05732966\n","Iteration 900, loss = 0.05716352\n","Iteration 901, loss = 0.05699803\n","Iteration 902, loss = 0.05683319\n","Iteration 903, loss = 0.05666898\n","Iteration 904, loss = 0.05650542\n","Iteration 905, loss = 0.05634249\n","Iteration 906, loss = 0.05618020\n","Iteration 907, loss = 0.05601854\n","Iteration 908, loss = 0.05585750\n","Iteration 909, loss = 0.05569709\n","Iteration 910, loss = 0.05553730\n","Iteration 911, loss = 0.05537813\n","Iteration 912, loss = 0.05521958\n","Iteration 913, loss = 0.05506164\n","Iteration 914, loss = 0.05490432\n","Iteration 915, loss = 0.05474760\n","Iteration 916, loss = 0.05459149\n","Iteration 917, loss = 0.05443598\n","Iteration 918, loss = 0.05428107\n","Iteration 919, loss = 0.05412676\n","Iteration 920, loss = 0.05397305\n","Iteration 921, loss = 0.05381993\n","Iteration 922, loss = 0.05366740\n","Iteration 923, loss = 0.05351546\n","Iteration 924, loss = 0.05336410\n","Iteration 925, loss = 0.05321332\n","Iteration 926, loss = 0.05306313\n","Iteration 927, loss = 0.05291351\n","Iteration 928, loss = 0.05276447\n","Iteration 929, loss = 0.05261600\n","Iteration 930, loss = 0.05246810\n","Iteration 931, loss = 0.05232077\n","Iteration 932, loss = 0.05217400\n","Iteration 933, loss = 0.05202779\n","Iteration 934, loss = 0.05188215\n","Iteration 935, loss = 0.05173706\n","Iteration 936, loss = 0.05159253\n","Iteration 937, loss = 0.05144855\n","Iteration 938, loss = 0.05130512\n","Iteration 939, loss = 0.05116223\n","Iteration 940, loss = 0.05101990\n","Iteration 941, loss = 0.05087810\n","Iteration 942, loss = 0.05073685\n","Iteration 943, loss = 0.05059613\n","Iteration 944, loss = 0.05045595\n","Iteration 945, loss = 0.05031631\n","Iteration 946, loss = 0.05017719\n","Iteration 947, loss = 0.05003861\n","Iteration 948, loss = 0.04990055\n","Iteration 949, loss = 0.04976301\n","Iteration 950, loss = 0.04962600\n","Iteration 951, loss = 0.04948951\n","Iteration 952, loss = 0.04935353\n","Iteration 953, loss = 0.04921807\n","Iteration 954, loss = 0.04908312\n","Iteration 955, loss = 0.04894868\n","Iteration 956, loss = 0.04881475\n","Iteration 957, loss = 0.04868133\n","Iteration 958, loss = 0.04854841\n","Iteration 959, loss = 0.04841599\n","Iteration 960, loss = 0.04828408\n","Iteration 961, loss = 0.04815265\n","Iteration 962, loss = 0.04802173\n","Iteration 963, loss = 0.04789130\n","Iteration 964, loss = 0.04776135\n","Iteration 965, loss = 0.04763190\n","Iteration 966, loss = 0.04750293\n","Iteration 967, loss = 0.04737445\n","Iteration 968, loss = 0.04724645\n","Iteration 969, loss = 0.04711893\n","Iteration 970, loss = 0.04699189\n","Iteration 971, loss = 0.04686532\n","Iteration 972, loss = 0.04673923\n","Iteration 973, loss = 0.04661361\n","Iteration 974, loss = 0.04648846\n","Iteration 975, loss = 0.04636378\n","Iteration 976, loss = 0.04623956\n","Iteration 977, loss = 0.04611581\n","Iteration 978, loss = 0.04599251\n","Iteration 979, loss = 0.04586968\n","Iteration 980, loss = 0.04574731\n","Iteration 981, loss = 0.04562539\n","Iteration 982, loss = 0.04550392\n","Iteration 983, loss = 0.04538291\n","Iteration 984, loss = 0.04526234\n","Iteration 985, loss = 0.04514223\n","Iteration 986, loss = 0.04502255\n","Iteration 987, loss = 0.04490333\n","Iteration 988, loss = 0.04478454\n","Iteration 989, loss = 0.04466620\n","Iteration 990, loss = 0.04454829\n","Iteration 991, loss = 0.04443082\n","Iteration 992, loss = 0.04431378\n","Iteration 993, loss = 0.04419718\n","Iteration 994, loss = 0.04408100\n","Iteration 995, loss = 0.04396526\n","Iteration 996, loss = 0.04384994\n","Iteration 997, loss = 0.04373504\n","Iteration 998, loss = 0.04362057\n","Iteration 999, loss = 0.04350652\n","Iteration 1000, loss = 0.04339289\n","[0 0 0 1]\n","1.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"}]},{"metadata":{"id":"OhHUVPJ1Lb2s","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}